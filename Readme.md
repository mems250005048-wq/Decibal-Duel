# ğŸ§ Conditional GAN for Audio Generation (CGAN) â€” README

This repository contains a full **Conditional GAN (CGAN)** pipeline that trains a model to generate audio samples from labeled environmental sound categories (e.g., dog bark, siren, drilling). The workflow includes automatic ZIP extraction, dataset discovery, preprocessing, model training, and audio reconstruction.

---

## ğŸ“ Project Overview

This project trains a **Conditional GAN** on log-mel spectrograms derived from `.wav` audio files. The generator learns to synthesize melâ€‘spectrograms conditioned on a chosen category, and the discriminator distinguishes real vs. fake mel-specs using spectral normalization.

Audio is reconstructed using **InverseMelScale** + **Griffin-Lim**.

---

## ğŸš€ Features

* Automatic ZIP extraction from Google Drive
* Auto-detection of category folder structure
* Dataset mean/std computation for normalization
* Logâ€‘mel spectrogram generator & discriminator (CGAN)
* Griffin-Lim phase reconstruction
* Audio saving & playback directly in Colab
* Training loop using **LSGAN loss**
* Fixed runtime error using `.reshape(-1)` instead of `.view(-1)`

---

## ğŸ“¦ Folder Requirements

Your extracted ZIP must contain category folders such as:

```
root_dir/
  â”œâ”€â”€ dog_bark/
  â”œâ”€â”€ drilling/
  â”œâ”€â”€ engine_idling/
  â”œâ”€â”€ siren/
  â””â”€â”€ street_music/
```

Each folder must contain `.wav` files.

---

## ğŸ”§ Installation

```bash
pip install torch torchaudio tqdm matplotlib
```

Google Colab will run this automatically.

---

## ğŸ“¥ ZIP Extraction

The code extracts your dataset from Google Drive:

```python
ZIP_PATH = "/content/drive/MyDrive/the-frequency-quest.zip"
EXTRACT_ROOT = "/content/data"
```

Make sure your ZIP is correctly located in your Drive.

---

## ğŸ§  Dataset Class

The dataset:

* Loads audio
* Converts stereo â†’ mono
* Computes logâ€‘mel spectrograms
* Normalizes using dataset mean/std
* Uses `.reshape(-1)` to prevent PyTorch runtime errors

---

## ğŸ— Model Architecture

### **Generator**

Takes:

* Random noise `z` (latent vector)
* Oneâ€‘hot label vector

Outputs a **1Ã—128Ã—512** mel-spectrogram.

### **Discriminator**

Receives:

* Mel-spectrogram
* Label embedding reshaped into a spatial map

Uses **spectral normalization** for training stability.

---

## ğŸ¹ Audio Reconstruction

Mel spectrogram â†’ Reconstructed waveform using:

* `InverseMelScale`
* `GriffinLim (n_iter=64)` for better audio clarity

Generated audio is saved as:

```
gan_generated_audio/<category>_ep<epoch>.wav
```

---

## ğŸ¯ Training

Training uses **Least Squares GAN (LSGAN)** to stabilize convergence.

Key hyperparameters:

```
LATENT_DIM = 100
EPOCHS = 300
BATCH_SIZE = 32
LR = 2e-4
```

After every 10 epochs, sample audio for the first 3 categories.

---

## ğŸ§ª Inference Example

To generate audio for category index 0:

```python
wav = generate_audio_gan(G, 0, DEVICE, train_dataset.mean, train_dataset.std)
save_and_play(wav, 22050, "example.wav")
```

---

## ğŸ›  Troubleshooting

### â— Dataset not found

Check the extracted folder structure and ensure category names match:

```python
train_categories = ['dog_bark', 'drilling', 'engine_idling', 'siren', 'street_music']
```

### â— Empty dataset

Ensure your category folders contain `.wav` files.

### â— Griffin-Lim slow or noisy

Decrease `n_iter`, but audio quality will drop.

---

## ğŸ“Œ Notes

* Modify category names to match your dataset.
* Adjust `max_frames` depending on your audio duration.
* Increase epochs for more realistic audio.
* Works best with clean audio datasets.

---

## ğŸ“„ License

MIT License. Feel free to modify and build on this! ğŸ˜Š

---

## ğŸ™Œ Credits

Developed for training CGANs on environmental sound datasets in Google Colab.

