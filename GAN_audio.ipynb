{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch torchaudio tqdm matplotlib > /dev/null\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, random, torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import zipfile\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# 0Ô∏è‚É£ File Extraction\n",
    "# =========================================================\n",
    "ZIP_PATH = \"/content/drive/MyDrive/the-frequency-quest.zip\"\n",
    "EXTRACT_ROOT = \"/content/data\"\n",
    "BASE_PATH = None\n",
    "print(f\"Attempting to extract {ZIP_PATH} to {EXTRACT_ROOT}...\")\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    os.makedirs(EXTRACT_ROOT, exist_ok=True)\n",
    "    try:\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(EXTRACT_ROOT)\n",
    "        print(\"‚úÖ Extraction complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå FATAL ERROR during extraction: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå FATAL ERROR: Zip file not found at {ZIP_PATH}. Check the path.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# 1Ô∏è‚É£ Dataset class\n",
    "# =========================================================\n",
    "class TrainAudioSpectrogramDataset(Dataset):\n",
    "    def __init__(self, root_dir, categories, max_frames=512, fraction=1.0, compute_stats=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.categories = categories\n",
    "        self.max_frames = max_frames\n",
    "        self.file_list = []\n",
    "        self.class_to_idx = {cat: i for i, cat in enumerate(categories)}\n",
    "\n",
    "        for cat_name in self.categories:\n",
    "            cat_dir = os.path.join(root_dir, cat_name)\n",
    "            if not os.path.isdir(cat_dir):\n",
    "                continue\n",
    "\n",
    "            files_in_cat = [os.path.join(cat_dir, f) for f in os.listdir(cat_dir) if f.lower().endswith(\".wav\")]\n",
    "            num_to_sample = int(len(files_in_cat) * fraction)\n",
    "            if num_to_sample == 0 and len(files_in_cat) > 0:\n",
    "                num_to_sample = 1\n",
    "            sampled_files = random.sample(files_in_cat, num_to_sample) if files_in_cat else []\n",
    "            label_idx = self.class_to_idx[cat_name]\n",
    "            self.file_list.extend([(file_path, label_idx) for file_path in sampled_files])\n",
    "\n",
    "        self.n_mels = 128\n",
    "        self.n_fft = 1024\n",
    "        self.hop_length = 128\n",
    "\n",
    "        # Compute dataset mean/std for normalization\n",
    "        self.mean = 0.0\n",
    "        self.std = 1.0\n",
    "        if compute_stats and len(self.file_list) > 0:\n",
    "            sums, sums_sq, count = 0.0, 0.0, 0\n",
    "            for path, _ in tqdm(self.file_list, desc=\"Computing dataset stats\", leave=False):\n",
    "                wav, sr = torchaudio.load(path)\n",
    "                if wav.size(0) > 1:\n",
    "                    wav = wav.mean(dim=0, keepdim=True)\n",
    "                mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "                    sample_rate=sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels\n",
    "                )(wav)\n",
    "                log_spec = torch.log1p(mel_spec)\n",
    "                _, _, n_frames = log_spec.shape\n",
    "                if n_frames < self.max_frames:\n",
    "                    log_spec = F.pad(log_spec, (0, self.max_frames - n_frames))\n",
    "                else:\n",
    "                    log_spec = log_spec[:, :, :self.max_frames]\n",
    "\n",
    "                # üî• FIX 1: Use reshape instead of view\n",
    "                v = log_spec.reshape(-1)\n",
    "\n",
    "                sums += v.sum().item()\n",
    "                sums_sq += (v * v).sum().item()\n",
    "                count += v.numel()\n",
    "            self.mean = sums / count\n",
    "            self.std = (sums_sq / count - (self.mean ** 2)) ** 0.5\n",
    "            print(f\"Dataset mean/std for log-mel: {self.mean:.6f} / {self.std:.6f}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.file_list[idx]\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if wav.size(0) > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels\n",
    "        )(wav)\n",
    "        log_spec = torch.log1p(mel_spec)\n",
    "        _, _, n_frames = log_spec.shape\n",
    "        if n_frames < self.max_frames:\n",
    "            log_spec = F.pad(log_spec, (0, self.max_frames - n_frames))\n",
    "        else:\n",
    "            log_spec = log_spec[:, :, :self.max_frames]\n",
    "\n",
    "        # üî• FIX 2: Use reshape instead of view (for consistency)\n",
    "        log_spec = (log_spec.reshape(1, self.n_mels, self.max_frames) - self.mean) / (self.std + 1e-9)\n",
    "\n",
    "        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n",
    "        return log_spec, label_vec\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# 2Ô∏è‚É£ Generator and Discriminator - generator part\n",
    "# =========================================================\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "class CGAN_Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_categories):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_categories = num_categories\n",
    "        self.fc = nn.Linear(latent_dim + num_categories, 256 * 8 * 32)\n",
    "        self.unflatten_shape = (256, 8, 32)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z, y):\n",
    "        h = torch.cat([z, y], dim=1)\n",
    "        h = self.fc(h).view(-1, *self.unflatten_shape)\n",
    "        return self.net(h)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# 2Ô∏è‚É£ Generator and Discriminator - discriminator part\n",
    "# =========================================================\n",
    "class CGAN_Discriminator(nn.Module):\n",
    "    def __init__(self, num_categories):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.label_embedding = nn.Linear(num_categories, 128 * 512)\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(2, 64, 4, 2, 1)), nn.LeakyReLU(0.2, inplace=True),\n",
    "            spectral_norm(nn.Conv2d(64, 128, 4, 2, 1)), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n",
    "            spectral_norm(nn.Conv2d(128, 256, 4, 2, 1)), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            spectral_norm(nn.Conv2d(256, 512, 4, 2, 1)), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.final_fc = None\n",
    "    def forward(self, x, labels):\n",
    "        label_map = self.label_embedding(labels).view(-1, 1, 128, 512)\n",
    "        d_in = torch.cat([x, label_map], dim=1)\n",
    "        out = self.model(d_in)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.final_fc is None:\n",
    "            self.final_fc = spectral_norm(nn.Linear(out.size(1), 1)).to(out.device)\n",
    "        return self.final_fc(out)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# 3Ô∏è‚É£ Audio generation helper (n_iter = 64 for better audio)\n",
    "# =========================================================\n",
    "def generate_audio_gan(generator, category_idx, device, dataset_mean, dataset_std,\n",
    "                       sample_rate=22050, n_fft=1024, hop_length=128, n_mels=128):\n",
    "    generator.eval()\n",
    "    y = F.one_hot(torch.tensor([category_idx]), num_classes=generator.num_categories).float().to(device)\n",
    "    z = torch.randn(1, generator.latent_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "        gen_norm_log = generator(z, y).squeeze(1).cpu()\n",
    "    gen_log = gen_norm_log * (dataset_std + 1e-9) + dataset_mean\n",
    "    mel_spec = torch.expm1(gen_log).clamp(min=1e-6)\n",
    "\n",
    "    inverse_mel = torchaudio.transforms.InverseMelScale(\n",
    "        n_stft=n_fft // 2 + 1, n_mels=n_mels, sample_rate=sample_rate,\n",
    "        driver=\"gelsy\"\n",
    "    )\n",
    "    griffin = torchaudio.transforms.GriffinLim(\n",
    "        n_fft=n_fft, hop_length=hop_length, win_length=n_fft, n_iter=64\n",
    "    )\n",
    "    linear_spec = inverse_mel(mel_spec)\n",
    "    wav = griffin(linear_spec).cpu().clamp(-1, 1)\n",
    "\n",
    "    return wav.unsqueeze(0)\n",
    "\n",
    "# =========================================================\n",
    "# 4Ô∏è‚É£ Save and Play helper\n",
    "# =========================================================\n",
    "def save_and_play(wav, sample_rate, filename):\n",
    "    os.makedirs(os.path.dirname(filename) or \".\", exist_ok=True)\n",
    "\n",
    "    if wav.dim() > 2:\n",
    "        wav_to_save = wav.squeeze(1)\n",
    "    else:\n",
    "        wav_to_save = wav\n",
    "\n",
    "    torchaudio.save(filename, wav_to_save, sample_rate)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "    display(Audio(wav_to_save.squeeze(0).numpy(), rate=sample_rate))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# 5Ô∏è‚É£ Training (LSGAN)\n",
    "# =========================================================\n",
    "def train_gan(generator, discriminator, dataloader, device, categories, epochs, lr, latent_dim, dataset_mean, dataset_std):\n",
    "    optG = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optD = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    os.makedirs(\"gan_generated_audio\", exist_ok=True)\n",
    "\n",
    "    hop_length = dataloader.dataset.hop_length\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for real_specs, labels in loop:\n",
    "            real_specs, labels = real_specs.to(device), labels.to(device)\n",
    "            bs = real_specs.size(0)\n",
    "            # --- D step ---\n",
    "            optD.zero_grad()\n",
    "            real_preds = discriminator(real_specs, labels)\n",
    "            z_G = torch.randn(bs, latent_dim, device=device)\n",
    "            fake_specs = generator(z_G, labels)\n",
    "            fake_preds = discriminator(fake_specs.detach(), labels)\n",
    "            lossD = 0.5 * ((real_preds - 1)**2 + (fake_preds)**2).mean()\n",
    "            lossD.backward(); optD.step()\n",
    "            # --- G step ---\n",
    "            optG.zero_grad()\n",
    "            z_D = torch.randn(bs, latent_dim, device=device)\n",
    "            fake_specs = generator(z_D, labels)\n",
    "            fake_preds = discriminator(fake_specs, labels)\n",
    "            lossG = 0.5 * ((fake_preds - 1)**2).mean()\n",
    "            lossG.backward(); optG.step()\n",
    "            loop.set_postfix(lossD=lossD.item(), lossG=lossG.item())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"\nGenerating audio for epoch {epoch}...\")\n",
    "            for i, cat in enumerate(categories[:3]):\n",
    "                wav = generate_audio_gan(generator, i, device, dataset_mean, dataset_std, hop_length=hop_length)\n",
    "                save_and_play(wav, 22050, f\"gan_generated_audio/{cat}_ep{epoch}.wav\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# 6Ô∏è‚É£ Run training\n",
    "# =========================================================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LATENT_DIM, EPOCHS, BATCH_SIZE, LR = 100, 300, 32, 2e-4\n",
    "\n",
    "# Correct category names (adjust if needed)\n",
    "train_categories = ['dog_bark', 'drilling', 'engine_idling', 'siren', 'street_music']\n",
    "\n",
    "# --- AUTOMATIC BASE PATH FINDER ---\n",
    "def find_category_base_path(root_dir, categories):\n",
    "    print(f\"\nSearching for category folders within {root_dir}...\")\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if all(cat in dirnames for cat in categories):\n",
    "            return dirpath\n",
    "    return None\n",
    "\n",
    "if os.path.isdir(EXTRACT_ROOT):\n",
    "    BASE_PATH = find_category_base_path(EXTRACT_ROOT, train_categories)\n",
    "\n",
    "# --- Start Training ---\n",
    "if BASE_PATH is None:\n",
    "    print(\"\nFATAL ERROR: Could not find the base directory containing all category folders.\")\n",
    "    print(f\"Please manually inspect the contents of {EXTRACT_ROOT} to determine the final path.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Automatically determined BASE_PATH: {BASE_PATH}\")\n",
    "    print(\"Categories:\", train_categories)\n",
    "\n",
    "    train_dataset = TrainAudioSpectrogramDataset(BASE_PATH, train_categories)\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"\nFATAL ERROR: Dataset is empty. Check that the extracted folders contain .wav files.\")\n",
    "    else:\n",
    "        print(f\"Dataset loaded successfully with {len(train_dataset)} samples.\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "        G = CGAN_Generator(LATENT_DIM, len(train_categories)).to(DEVICE)\n",
    "        D = CGAN_Discriminator(len(train_categories)).to(DEVICE)\n",
    "        print(\"Generator output shape:\", G(torch.randn(1, LATENT_DIM).to(DEVICE),\n",
    "                                             F.one_hot(torch.tensor([0]), num_classes=len(train_categories)).float().to(DEVICE)).shape)\n",
    "\n",
    "        train_gan(G, D, train_loader, DEVICE, train_categories, EPOCHS, LR, LATENT_DIM, train_dataset.mean, train_dataset.std)\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
